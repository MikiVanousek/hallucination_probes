# Meditron Dataset Creation Script

This script creates a conversation dataset by combining the PubMedQA dataset with responses generated by the OpenMeditron/Meditron3-8B model. The resulting dataset is formatted to be compatible with the annotation pipeline described in the main README.

## Overview

The script:
1. Loads the PubMedQA dataset from HuggingFace
2. Uses the OpenMeditron/Meditron3-8B model to generate medical responses
3. Formats conversations according to the annotation pipeline requirements
4. Saves the dataset locally and optionally pushes it to HuggingFace under username MikiV

## Installation

Install the required dependencies:

```bash
pip install -r requirements_meditron.txt
```

## Usage

### Basic Usage

Generate a small test dataset (100 samples) and save locally:

```bash
python meditron_dataset_creation.py --max-samples 100 --no-push
```

### Full Dataset Generation

Generate and push to HuggingFace:

```bash
python meditron_dataset_creation.py \
    --max-samples 1000 \
    --dataset-name pubmedqa-meditron-conversations \
    --username MikiV
```

### Command Line Options

- `--model`: Meditron model to use (default: "OpenMeditron/Meditron3-8B")
- `--max-samples`: Maximum number of samples to process (default: all)
- `--output-dir`: Output directory for local files (default: "./meditron_conversations")
- `--dataset-name`: Name for the HuggingFace dataset (default: "pubmedqa-meditron-conversations")
- `--username`: HuggingFace username (default: "MikiV")
- `--no-push`: Don't push to HuggingFace, only save locally
- `--pubmedqa-subset`: PubMedQA subset to use (default: "pqa_labeled")

## Output Format

The generated dataset follows the annotation pipeline format with each item containing:

```json
{
  "conversation": [
    {"role": "user", "content": "Medical question from PubMedQA"},
    {"role": "assistant", "content": "Generated response from Meditron"}
  ],
  "pubmedqa_id": "original_pubmed_id",
  "original_answer": "original_pubmedqa_answer",
  "context": "truncated_context_from_abstract",
  "source": "pubmedqa_meditron"
}
```

## HuggingFace Authentication

To push datasets to HuggingFace, you need to authenticate:

```bash
# Option 1: Set environment variable
export HUGGINGFACE_HUB_TOKEN="your_token_here"

# Option 2: Login via CLI
huggingface-cli login
```

## Example Runs

See `run_meditron_example.sh` for complete examples of how to use the script.

## Hardware Requirements

- GPU with at least 16GB VRAM recommended for Meditron3-8B
- Sufficient disk space for the model and generated datasets
- Internet connection for downloading models and datasets

## Troubleshooting

### Out of Memory Issues
- Reduce `--max-samples` to process smaller batches
- Use a smaller model variant if available
- Ensure sufficient GPU memory

### Model Loading Issues
- Verify internet connection for model download
- Check HuggingFace authentication if using gated models
- Ensure sufficient disk space for model files

### Dataset Upload Issues
- Verify HuggingFace authentication
- Check dataset name doesn't conflict with existing datasets
- Ensure sufficient network bandwidth for upload

## Integration with Annotation Pipeline

The generated dataset can be directly used with the main annotation pipeline:

```bash
# Generate the dataset
python meditron_dataset_creation.py --max-samples 500

# Run annotation pipeline
python annotation_pipeline.py \
    --dataset MikiV/pubmedqa-meditron-conversations \
    --output annotated_meditron_conversations
```

This creates a complete pipeline from PubMedQA → Meditron responses → Claude annotations.