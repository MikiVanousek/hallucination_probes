# meditron.yaml

probe_config:
  probe_id: "meditron3_8b_lora_probe"

  # Use the Meditron model you cited
  model_name: "OpenMeditron/Meditron3-8B"

  # Safer than hard-coding: compute ~95% depth (works if your trainer supports it).
  # If your trainer REQUIRES an int, set 30 for 32-layer models (second-to-last).
  layer: 30

  # LoRA over standard LLaMA-family targets; "all" is often NOT a valid value.
  lora_layers:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - up_proj
    - down_proj
    - gate_proj
  lora_r: 32
  lora_alpha: 64
  lora_dropout: 0.05

  load_from: null
  hf_repo_id: "obalcells/hallucination-probes"

  # Start a bit lower; 0.35–0.4 usually yields nicer abstention curves
  threshold: 0.38

# Logging & exports
wandb_project: "hallucination-probes"
upload_to_hf: false
save_evaluation_metrics: true
save_roc_curves: false
dump_raw_eval_results: false

# ---------- Training hyperparameters ----------
per_device_train_batch_size: 2          # 8B + 1.5k ctx → 2 is safer than 4
per_device_eval_batch_size: 2
enable_gradient_checkpointing: true
gradient_accumulation_steps: 8          # keep effective batch reasonable on 8B
max_grad_norm: 1.0
logging_steps: 10
seed: 42

# Loss / regularization
high_loss_threshold: null
lambda_lm: 0.0
lambda_kl: 0.5                          # keep behavior close to base model
anneal_max_aggr: true                   # span-max annealing on
anneal_warmup: 0.1                      # 10% of total steps; change if your trainer expects steps

# Optimizers
probe_head_lr: 1.50e-3
lora_lr: 3.0e-4                          # slightly higher than 1e-4 speeds convergence without drift
weight_decay: 0.01
lr_scheduler: "cosine"
warmup_ratio: 0.08

# ---------- Training datasets ----------
# Important: keep using existing annotations, but RETOKENIZE with Meditron’s tokenizer.
# If your pipeline supports it, set a global or per-dataset flag like `retokenize_with_model: true`.

train_datasets:
  - dataset_id: "llama3_1_8b_longfact_train"
    hf_repo: "obalcells/longfact-annotations"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "train"
    max_length: 1536
    default_ignore: false
    last_span_token: false
    ignore_buffer: 0
    pos_weight: 10.0
    neg_weight: 1.0
    shuffle: true
    seed: 42
    process_on_the_fly: false
    retokenize_with_model: true

  - dataset_id: "llama3_1_8b_longfact_augmented_train"
    hf_repo: "obalcells/longfact-augmented-annotations"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "train"
    max_length: 1536
    default_ignore: false
    last_span_token: false
    ignore_buffer: 0
    pos_weight: 10.0
    neg_weight: 1.0
    shuffle: true
    seed: 42
    process_on_the_fly: false
    retokenize_with_model: true

  - dataset_id: "llama3_3_70b_longfact_train"
    hf_repo: "obalcells/longfact-annotations"
    subset: "Llama-3.3-70B-Instruct"
    split: "train"
    max_length: 1536
    default_ignore: false
    last_span_token: false
    ignore_buffer: 0
    pos_weight: 10.0
    neg_weight: 1.0
    shuffle: true
    seed: 42
    process_on_the_fly: false
    retokenize_with_model: true

  - dataset_id: "llama3_3_70b_longfact_augmented_train"
    hf_repo: "obalcells/longfact-augmented-annotations"
    subset: "Llama-3.3-70B-Instruct"
    split: "train"
    max_length: 1536
    default_ignore: false
    last_span_token: false
    ignore_buffer: 0
    pos_weight: 10.0
    neg_weight: 1.0
    shuffle: true
    seed: 42
    process_on_the_fly: false
    retokenize_with_model: true

  - dataset_id: "qwen2_5_7b_longfact_augmented_train"
    hf_repo: "obalcells/longfact-augmented-annotations"
    subset: "Qwen2.5-7B-Instruct"
    split: "train"
    max_length: 1536
    default_ignore: false
    last_span_token: false
    ignore_buffer: 0
    pos_weight: 10.0
    neg_weight: 1.0
    shuffle: true
    seed: 42
    process_on_the_fly: false
    retokenize_with_model: true

  - dataset_id: "gemma2_9b_longfact_augmented_train"
    hf_repo: "obalcells/longfact-augmented-annotations"
    subset: "gemma-2-9b-it"
    split: "train"
    max_length: 1536
    default_ignore: false
    last_span_token: false
    ignore_buffer: 0
    pos_weight: 10.0
    neg_weight: 1.0
    shuffle: true
    seed: 42
    process_on_the_fly: false
    retokenize_with_model: true

  - dataset_id: "mistral_small_24b_longfact_augmented_train"
    hf_repo: "obalcells/longfact-augmented-annotations"
    subset: "Mistral-Small-24B-Instruct-2501"
    split: "train"
    max_length: 1536
    default_ignore: false
    last_span_token: false
    ignore_buffer: 0
    pos_weight: 10.0
    neg_weight: 1.0
    shuffle: true
    seed: 42
    process_on_the_fly: false
    retokenize_with_model: true

  - dataset_id: "llama3_1_8b_trivia_qa_train"
    hf_repo: "obalcells/triviaqa-balanced"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "train"
    max_length: 1536
    default_ignore: true
    last_span_token: false
    ignore_buffer: 0
    pos_weight: 10.0
    neg_weight: 1.0
    shuffle: true
    seed: 42
    process_on_the_fly: false
    retokenize_with_model: true

# ---------- Evaluation datasets ----------
eval_datasets:
  - dataset_id: "llama3_1_8b_longform_test"
    hf_repo: "obalcells/longfact-annotations"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "test"
    max_length: 1536
    pos_weight: 10.0
    neg_weight: 1.0
    default_ignore: false
    shuffle: false
    retokenize_with_model: true

  - dataset_id: "llama3_1_8b_longform_augmented_test"
    hf_repo: "obalcells/longfact-augmented-annotations"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "test"
    max_length: 1536
    pos_weight: 10.0
    neg_weight: 1.0
    default_ignore: false
    shuffle: false
    retokenize_with_model: true

  - dataset_id: "llama3_1_8b_trivia_qa_test"
    hf_repo: "obalcells/triviaqa-balanced"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "test"
    max_length: 128
    pos_weight: 10.0
    neg_weight: 1.0
    default_ignore: false
    shuffle: false
    retokenize_with_model: true

  - dataset_id: "llama3_1_8b_healthbench_test"
    hf_repo: "obalcells/healthbench-annotations"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "test"
    max_length: 1536
    pos_weight: 1.0
    neg_weight: 1.0
    default_ignore: false
    shuffle: false
    retokenize_with_model: true
