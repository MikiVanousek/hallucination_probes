probe_config:
  probe_id: "meditron3_8b_lora_probe"
  model_name: "OpenMeditron/Meditron3-8B"
  layer: 30
  threshold: 0.46   # change to sweep different operating points

# REQUIRED/ACCEPTED by your EvaluationConfig
per_device_eval_batch_size: 2
output_dir: "value_head_probes/meditron3_8b_lora_probe"
save_roc_curves: false
save_raw_results: false
# (save_predictions exists in your schema but is optional; leave false by default)

# IMPORTANT: use the key name 'datasets' (NOT eval_datasets)
datasets:
  - dataset_id: "llama3_1_8b_longform_test"
    hf_repo: "obalcells/longfact-annotations"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "test"
    max_length: 1536

  - dataset_id: "llama3_1_8b_longform_augmented_test"
    hf_repo: "obalcells/longfact-augmented-annotations"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "test"
    max_length: 1536

  - dataset_id: "llama3_1_8b_trivia_qa_test"
    hf_repo: "obalcells/triviaqa-balanced"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "test"
    max_length: 128

  - dataset_id: "llama3_1_8b_healthbench_test"
    hf_repo: "obalcells/healthbench-annotations"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "test"
    max_length: 1536
